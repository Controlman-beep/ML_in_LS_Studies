{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\n# ms-python.python added\nimport os\ntry:\n\tos.chdir(os.path.join(os.getcwd(), '..'))\n\tprint(os.getcwd())\nexcept:\n\tpass\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" #Title: \"XO\"\n"," #Author: \"Merghadi Abdelaziz\""],"metadata":{}},{"cell_type":"markdown","source":[" # Import Libraries"],"metadata":{}},{"source":["# Essentials\n","import numpy as np \n","import pandas as pd \n","import tables\n","from functools import partial\n","\n","# For Ploting\n","import matplotlib.pyplot as plt\n","from cycler import cycler\n","import matplotlib as mpl\n","import matplotlib.gridspec as gridspec\n","from matplotlib.animation import FuncAnimation\n","import seaborn as sns\n","from IPython.display import HTML\n","%matplotlib inline\n","from rasterio import rio\n","from rasterio.plot import show\n","\n","# For Scipy\n","from scipy import interp\n","from scipy.stats import wilcoxon, mannwhitneyu, friedmanchisquare, ttest_ind\n","\n","# For Sci-Kit Learn\n","from sklearn.feature_selection import mutual_info_classif\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n","from lightgbm import LGBMClassifier\n","# from catboost import CatBoostClassifier\n","\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import BernoulliNB, GaussianNB\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier \n","from sklearn.neural_network import MLPClassifier\n","from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n","\n","from sklearn.model_selection import cross_validate, cross_val_score,cross_val_predict, KFold, StratifiedKFold, train_test_split\n","from sklearn.metrics import accuracy_score, roc_auc_score, cohen_kappa_score, log_loss, make_scorer, auc, roc_curve,brier_score_loss\n","from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n","import random\n","\n","# For HyperOpt\n","from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, rand, space_eval\n","from hyperopt.pyll import scope as hp_scope\n","from hyperopt.pyll.stochastic import sample as ho_sample\n","\n","# OS Specific\n","from pathlib import Path\n","import time\n","\n","# Geo-Data\n","import geopandas as gpd \n","import rasterio as rio\n","from osgeo import gdal,ogr\n","\n","#For Markdown\n","from tabulate import tabulate\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Init. Project Folder Path \n","projhome = Path.cwd()\n","\n","# Number of CPUs to use in Optimization\n","cpus = 2\n","\n","# Set the RandomState of the seed:\n","seed = 101\n","rng = np.random.RandomState(seed)\n","np.random.seed(seed)         \n","print (\"Random number with seed 30\")\n","random.seed(seed)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Load Input Data\n","def load_data(data):\n","    try:\n","        df = pd.read_csv(data)\n","        return df\n","    except IOError:\n","        print(\"IOError: The file is not available...Check if it exist!\")\n","\n","\n","# Dataset Path\n","data_path = \"./Data/Input_Data_Array.csv\"\n","input_df = load_data(data_path)\n","input_df = input_df.iloc[:,1:]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Compute the correlation matrix\n","input_cor = input_df.drop(\"Landslides\", axis=1).corr()\n","\n","# Compute info-Gain\n","gain_df = {\"variable\":input_df.drop(\"Landslides\", axis=1).columns,\n","                        \"gain\":mutual_info_classif(input_df.drop(\"Landslides\", axis=1), input_df[\"Landslides\"], discrete_features=True)}\n","gain_df = pd.DataFrame(data=gain_df)\n","with open(\"./Output/Tables/gain_df.md\",'w') as tabl:\n","    print(tabulate(gain_df, headers=['variable', 'gain'],tablefmt=\"grid\"),file=tabl)\n","\n","plt.style.use(\"seaborn-paper\")\n","fig, ax = plt.subplots(figsize=(5,5))\n","gain_df.plot.barh(x=\"variable\",y=\"gain\",ax=ax,legend=False)\n","ax.xaxis.grid(False)\n","ax.yaxis.grid(False)\n","fig.tight_layout()\n","plt.savefig('./Output/Figures/gain_plot.jpg',quality=100, dpi=300,bbox_inchs=\"tight\")\n","\n","# Compute the VIF\n","vifs = pd.DataFrame(data={\"vif\":np.linalg.inv(input_cor.values).diagonal(),\"variable\":input_cor.index})\n","# vifs = pd.Series(np.linalg.inv(input_cor.values).diagonal(), index=input_cor.index)\n","# vifs.astype(\"float\")\n","with open(\"./Output/Tables/vif_df.md\",'w') as tabl:\n","    print(tabulate(vifs, headers=['variable', 'vif'],tablefmt=\"grid\"),file=tabl)\n","\n","plt.style.use(\"seaborn-paper\")\n","fig, ax = plt.subplots(figsize=(5,5))\n","vifs.plot.barh(x=\"variable\",y=\"vif\",ax=ax,legend=False)\n","ax.xaxis.grid(False)\n","ax.yaxis.grid(False)\n","fig.tight_layout()\n","plt.savefig('./Output/Figures/vif_plot.jpg',quality=100, dpi=300,bbox_inchs=\"tight\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # Init. the necessary functions"],"metadata":{}},{"source":["# Init. the splitting function:\n","def cv_splitting(features, labels, cv, inner_cv=None):\n","    '''\n","    0 Refer to Features\n","    1 Refer to Labels\n","    '''\n","    X_train_out = []\n","    Y_train_out = []\n","    X_test_out = []\n","    Y_test_out = []\n","    print(\"Splitting the Input data according to The resampling...\")\n","    for i, (train_idx, test_idx) in enumerate(cv.split(features, labels)):\n","        print(\"Fold number %d....\" % (i), end=\" \")\n","        X_train_out.append(features[train_idx])\n","        Y_train_out.append(labels[train_idx])\n","        X_test_out.append(features[test_idx])\n","        Y_test_out.append(labels[test_idx])\n","        print(\"done.\")\n","    return X_train_out,Y_train_out,X_test_out,Y_test_out\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Setting up the objective function\n","count = 0\n","def objective_func(params,xdata, clf, ydata, cv_resample,metric=\"roc_auc\"):\n","    global count\n","    count += 1\n","    lrn_clf = clf\n","    lrn_clf = lrn_clf.set_params(**params)\n","    obj_scores = []\n","    for i, (X, Y) in enumerate(zip(xdata, ydata)):\n","        cv_score = cross_val_score(lrn_clf, X=X, y=Y, scoring=metric, cv=cv_resample,error_score='raise').mean()\n","        if cv_score is not None:\n","            obj_scores.append(cv_score)\n","\n","    loss = -np.mean(obj_scores)\n","    print('''\\n#############################\\n Results ƪ(˘⌣˘)ʃ \\n#############################\\n\\n\\\n","    Iterations: %s \\n\\\n","    Loss: %.5f \\n\\\n","    using: %s \\n#############################\\n''' % (count, loss, params))\n","    return {'loss': loss, 'status': STATUS_OK}\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Output the optimization trails function\n","def optimization_trails(trails,search_space):\n","\n","    best_hp = dict([(k,np.NaN) if not v else (k,v[0]) for k,v in trails.best_trial[\"misc\"][\"vals\"].items()])\n","    best_hp = space_eval(search_space,best_hp)\n","    best_hp[\"tid\"] = trails.best_trial[\"misc\"][\"tid\"]\n","    best_hp[\"loss\"] = trails.best_trial[\"result\"][\"loss\"]\n","    best_hp[\"status\"] = trails.best_trial[\"result\"][\"status\"]\n","    best_hp = pd.DataFrame.from_dict([best_hp])\n","\n","    optimization_df = []\n","    for idx, value in enumerate(trails.trials):\n","        optimization_dic = dict([(k,np.NaN) if not v else (k,v[0]) for k,v in value[\"misc\"][\"vals\"].items()])\n","        optimization_dic = space_eval(search_space,optimization_dic)\n","        optimization_dic[\"tid\"] = value[\"misc\"][\"tid\"]\n","        optimization_dic[\"loss\"] = value[\"result\"][\"loss\"]\n","        optimization_dic[\"status\"] = value[\"result\"][\"status\"]\n","        optimization_df.append(optimization_dic)\n","\n","    optimization_df = pd.DataFrame(optimization_df)  \n","    return optimization_df, best_hp\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Plotting optimization history function\n","def optimization_history(trails):\n","    fig, ax = plt.subplots(figsize=(6, 6))\n","    ax.scatter(range(1, len(trails) + 1),\n","                [-x['result']['loss'] for x in trails],\n","                edgecolor='black',c=\"C1\", linewidth=1,\n","                s=80, zorder=2, label=\"Iteration\")\n","\n","    ax.set_xlabel('Iteration', fontsize=12)\n","    # ax.set_xticks(range(1, len(trails) + 1,1))\n","    ax.set_ylabel('ROC-AUC', fontsize=12)\n","    ax.set_title('Optimization history', fontsize=14)\n","    ax.grid(\"on\", linestyle='--', linewidth=1, alpha=0.3)\n","    return fig, ax\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Train and evaluate the final model\n","def tunned_model_evaluation(clf,opt_hp,x,y,cv,scores):\n","    print(f\"Train & and validate using the Following Parameters: \\n {opt_hp}\")  \n","    lrn_clf = clf\n","    lrn_clf = lrn_clf.set_params(**opt_hp)\n","\n","    # generate scores metrics\n","    final = cross_validate(lrn_clf, X=x, y=y, cv=cv,scoring=scores, return_train_score=True)\n","    # final_prob = cross_val_predict(lrn_clf, X=x, y=y, cv=cv,method=\"predict_proba\")\n","\n","    # Formating the exported scores\n","    scores_raw = pd.DataFrame(data=final)\n","    scores = pd.DataFrame({\"mean\":np.mean(scores_raw), \"std\":np.std(scores_raw)})\n","    return scores, scores_raw\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Significance test and Pair-wise analysis"],"metadata":{}},{"source":["# Wilcoxon Pair-wise analysis\n","def sig_stat(data_list,data_names):\n","\n","    pval = []\n","    statval = []\n","    pair = []\n","\n","    for idx in range(0,len(data_list),1):\n","        if idx < len(data_list):\n","            i = idx+1\n","            while i < len(data_list):\n","                t, p = ttest_ind(data_list[idx], data_list[i], equal_var = False)\n","                # t, p = mannwhitneyu(data_list[idx].values, data_list[i].values)\n","                statval.append(round(t,3))\n","                pval.append(round(p,4))\n","                pair.append(f\"{data_names[idx]} vs. {data_names[i]}\")\n","                # print(f\"{idx}     {i}\")\n","                print(f\"{data_names[idx]} vs. {data_names[i]}:\\nT: {t} \\t p: {p}\")\n","                i += 1\n","            else:\n","                continue\n","        else:\n","            break\n","    return pd.DataFrame({\"Pairwise\":pair,\"p.value\":pval,\"t.value\":statval})\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["## Classification and ROC analysis"],"metadata":{}},{"source":["# Run classifier with cross-validation and plot ROC curves\n","# Compute ROC curve and area the curve function\n","def generate_roc_data(clf,train_x,train_y,test_x,test_y):\n","    tprs = []\n","    aucs = []\n","    mean_fpr = np.linspace(0, 1, 100)\n","    for x,y,xt,yt in zip(train_x,train_y,test_x,test_y):\n","        probas_ = clf.fit(x, y).predict_proba(xt)\n","        # Compute ROC curve and area the curve\n","        fpr, tpr, _ = roc_curve(yt, probas_[:, 1])\n","        tprs.append(interp(mean_fpr, fpr, tpr))\n","        tprs[-1][0] = 0.0\n","        roc_auc = auc(fpr, tpr)\n","        aucs.append(roc_auc)\n","        mean_tpr = np.mean(tprs, axis=0)\n","        mean_tpr[-1] = 1.0\n","        mean_auc = auc(mean_fpr, mean_tpr)\n","        std_auc = np.std(aucs)\n","        std_tpr = np.std(tprs, axis=0)\n","        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n","        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n","    return (fpr, tpr, roc_auc), (mean_fpr, mean_tpr, mean_auc, std_auc,tprs_upper,tprs_lower,std_tpr)\n","# Plot all roc plots function\n","def plot_roc_curves(roc_data,roc_data_names):\n","    mpl.style.use(\"seaborn-paper\")\n","    # custom_cycler = cycler('linestyle',['-','--',':','-.'])\n","    custom_cycler = (cycler(color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n","                                '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n","                                '#bcbd22', '#17becf']) + \n","                                cycler(linestyle=['-','--',':','-.','-','--',':','-.','-','--'])\n","                                )\n","    fig, ax = plt.subplots(figsize=(6, 6))\n","    ax.plot([0, 1], [0, 1], linestyle=':', lw=0.8, alpha=.8,\n","                color='k',\n","                # label='Luck'\n","                )\n","    ax.set_prop_cycle(custom_cycler)\n","    # return (fpr, tpr, roc_auc), (mean_fpr, mean_tpr, mean_auc, std_auc,tprs_upper,tprs_lower,std_tpr)\n","    for idx, value in enumerate(roc_data):\n","        ax.plot(value[0], value[1], \\\n","                    # color='C1',\n","                    label=f\"{roc_data_names[idx]} AUC-ROC = {value[2]:0.3f} +/- {value[3]:0.3f}\",\n","                    lw=1.4)\n","                    # lw=1.4, alpha=.8)\n","        # ax.fill_between(value[0], value[5], value[4],\n","        #             # label=f\"+/- 1 std. dev.\",\n","        #             # color='grey',\n","        #             alpha=.1\n","        #             )\n","\n","    ax.set_xlim([-0.05, 1.05])\n","    ax.set_ylim([-0.05, 1.05])\n","    ax.set_xlabel('False Positive Rate')\n","    ax.set_ylabel('True Positive Rate')\n","    # ax.set_title('Receiver operating characteristic curve')\n","    ax.legend(loc=\"lower right\")\n","    return fig, ax\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Calibration analysis"],"metadata":{}},{"source":["# Probability Calibration curves function\n","def plot_calibration_curves(clf,x,y,test_size,cv, seed, clf_name):\n","    from sklearn.metrics import brier_score_loss\n","    custom_cycler = (cycler(color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n","                                '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n","                                '#bcbd22', '#17becf']) + \n","                                cycler(marker=['o', 'X', 's', '8', '>', '*',\"h\",\"v\",\"p\",\"d\"]) +\n","                                cycler(linestyle=['-','--',':','-.','-','--',':','-.','-','--'])\n","                                )\n","    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size,random_state=101)\n","    \n","    fig,ax1 = plt.subplots(figsize=(7, 7))\n","    # fig,(ax1,ax2) = plt.subplots(2,1,figsize=(8, 8))\n","    ax1.set_prop_cycle(custom_cycler)\n","    # ax2.set_prop_cycle(custom_cycler)\n","    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n","\n","    for lrn, lrn_name in zip(clf, clf_name):\n","        \"\"\"Plot calibration curve for clf w/o and with calibration. \"\"\"\n","        # Calibrated with isotonic calibration\n","        isotonic = CalibratedClassifierCV(lrn, cv=cv, method='isotonic')\n","        # Calibrated with sigmoid calibration\n","        sigmoid = CalibratedClassifierCV(lrn, cv=cv, method='sigmoid')\n","        # Logistic regression with no calibration as baseline\n","        # lr = LogisticRegression(C=1., solver='lbfgs')\n","        \n","        def generate_calibration_data(model,X_train,y_train,X_test,y_test):\n","            model.fit(X_train, y_train)\n","            # y_pred = lrn_clf.predict(X_test)\n","            if hasattr(model, \"predict_proba\"):\n","                prob_pos = model.predict_proba(X_test)[:, 1]\n","            else:  # use decision function\n","                prob_pos = model.decision_function(X_test)\n","                prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n","            \n","            fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=10)       \n","            lrn_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())\n","            return fraction_of_positives, mean_predicted_value, lrn_score, prob_pos\n","\n","\n","        for lrn_clf, name in [(lrn, lrn_name)\n","                        # (isotonic, lrn_name + ' + Isotonic'),\n","                        # (sigmoid, lrn_name + ' + Sigmoid')\n","                        ]:\n","            fop, mpv, lss, pp = generate_calibration_data(lrn_clf,X_train,y_train,X_test,y_test)\n","            # ax1.plot(mpv, fop,alpha=0.75,label=f\"{name} {lss:1.3f}\")\n","            ax1.plot(mpv, fop,label=f\"{name}\")\n","            # ax2.hist(pp, range=(0, 1),linestyle=\"-\", bins=10,\\\n","            #  label=name,histtype=\"step\",alpha=0.75, lw=2)\n","\n","    ax1.set_ylabel(\"Fraction of positives\")\n","    ax1.set_xlabel(\"Mean predicted value\")\n","    ax1.set_ylim([-0.05, 1.05])\n","    ax1.legend(loc=\"lower right\")\n","\n","    # ax1.set_title('Calibration plots  (reliability curve)')\n","\n","    # ax2.set_xlabel(\"Mean predicted value\")\n","    # ax2.set_ylabel(\"Count\")\n","    # ax2.legend(loc=\"upper center\", ncol=2)\n","    # mpl.style.use(\"seaborn-paper\")\n","    fig.tight_layout()\n","    # return fig, (ax1,ax2)\n","    return fig, (ax1)\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Prediction analysis"],"metadata":{}},{"source":["def perform_prediction(clf,clf_names, train_data,predict_data,bins=None,bins_labels=None,XY=None,export_prediction=False,export_csv=False):\n","    prediction = []\n","    prediction_classified = []\n","    freq_summary = []\n","    for lrn_clf, lrn_clf_name in zip(clf,clf_names) :\n","        # print(f\"Train & and validate using the Following Parameters:\")  \n","        lrn_clf_prediction = lrn_clf.fit(train_data[0], train_data[1]).predict_proba(predict_data)[:,-1]\n","        prediction.append(lrn_clf_prediction)\n","\n","        if bins is not None:\n","            bins = np.array(bins)\n","            lrn_clf_prediction_classified = np.digitize(lrn_clf_prediction, bins)\n","            prediction_classified.append(lrn_clf_prediction_classified)\n","\n","            unique_class, unique_count = np.unique(lrn_clf_prediction_classified,return_counts=True,axis=0)\n","            model = np.repeat(lrn_clf_name,len(unique_class))\n","            unique_count_percentage = (unique_count / np.sum(unique_count,axis=0))*100\n","            freq_summary.append(np.column_stack((unique_class,unique_count, unique_count_percentage,model)))\n","\n","    freq_summary = np.row_stack((freq_summary))\n","    freq_summary = pd.DataFrame(data=freq_summary,columns=[\"class\",\"count\",\"percentage\",\"model\"])\n","    freq_summary[\"class\"] = freq_summary[\"class\"].astype(str)\n","    freq_summary[\"count\"] = freq_summary[\"count\"].astype(int)\n","    freq_summary[\"percentage\"] = freq_summary[\"percentage\"].astype(float)\n","    freq_summary = freq_summary.round({\"percentage\":3})\n","\n","    if XY is not None:\n","        if export_prediction is True:\n","            prediction = np.column_stack((prediction))\n","            prediction = np.column_stack((XY,prediction))\n","            col_names =[\"x\",\"y\"] + clf_names\n","            prediction = pd.DataFrame(data=prediction,columns=col_names)\n","            if export_csv is True:\n","                prediction.to_csv(\"./Output/Tables/prediction.csv\",index=False)\n","        else: \n","            prediction_classified = np.column_stack((prediction_classified))\n","            prediction_classified = np.column_stack((XY,prediction_classified))\n","            col_names =[\"x\",\"y\"] + clf_names\n","            prediction_classified = pd.DataFrame(data=prediction_classified,columns=col_names)\n","            if export_csv is True:\n","                prediction_classified.to_csv(\"./Output/Tables/prediction.csv\",index=False)\n","            # return prediction_classified, freq_summary\n","\n","    else:\n","        pass\n","\n","    return prediction, prediction_classified, freq_summary\n","\n","\n","def prediction_to_raster(clf_names,vrt_file,layers,outputSRS=None,outputBounds=None,width=None,height=None,noData=None):\n","\n","    for lrn_clf_name in clf_names:\n","        output_file = \"./Output/Figures/\" + lrn_clf_name + \".tif\"\n","        gridopt = gdal.GridOptions(format='GTiff',\n","        zfield=lrn_clf_name,layers=layers,\n","                        noData=noData,\n","                        # outputType=gdal.GDT_Int16,\n","                        outputSRS=outputSRS,\n","                        outputBounds=outputBounds,\n","                        width=width,height=height,\n","                        algorithm='nearest')\n","        output = gdal.Grid(output_file,vrt_file,options=gridopt)\n","        # output.FlushCache()\n","        output = None\n","\n","        wrap_output_file = \"./Output/Figures/\" + lrn_clf_name + \"_rs.tif\"\n","        wrapopt = gdal.WarpOptions(outputBoundsSRS=outputSRS\n","                                    # xRes=50,yRes=50,\n","                                    )\n","        output = gdal.Warp(wrap_output_file,output_file,\n","                options=wrapopt\n","                # outputType=gdal.GDT_Int16,\n","                # xRes=45, yRes=45\n","                )\n","        # output.FlushCache()\n","        output = None\n","\n","        # Replace wrapped file by the old one\n","        Path(output_file).unlink()\n","        Path(wrap_output_file).rename(output_file)\n","\n","def extract_rs_unique_values(sampling_shp, raster,raster_names):\n","    summary = []\n","    for raster, raster_name in zip(rasters_list, models_names_list):\n","\n","        src_ds=gdal.Open(raster) \n","        gt=src_ds.GetGeoTransform()\n","        rb=src_ds.GetRasterBand(1)\n","\n","        ds=ogr.Open(sampling_shp)\n","        lyr=ds.GetLayer()\n","        summary_raster = []\n","        for feat in lyr:\n","            geom = feat.GetGeometryRef()\n","            mx,my=geom.GetX(), geom.GetY()  #coord in map units\n","\n","            #Convert from map to pixel coordinates.\n","            #Only works for geotransforms with no rotation.\n","            px = int((mx - gt[0]) / gt[1]) #x pixel\n","            py = int((my - gt[3]) / gt[5]) #y pixel\n","\n","            intval=rb.ReadAsArray(px,py,1,1)\n","            if intval is None:\n","                intval = np.array([[4]])\n","            # intval=[4 if i is None else i for i in rb.ReadAsArray(px,py,1,1)]\n","            # print(intval[0]) #intval is a numpy array, length=1 as we only asked for 1 pixel value\n","            summary_raster.append(intval[0])\n","\n","        summary_raster = np.row_stack((summary_raster))\n","        unique_class, unique_count = np.unique(summary_raster,return_counts=True,axis=0)\n","        unique_count_percentage = (unique_count / np.sum(unique_count,axis=0)) * 100\n","        model = np.repeat(raster_name,len(unique_class))\n","        summary.append(np.column_stack((unique_class,unique_count, unique_count_percentage,model)))\n","\n","    summary = np.row_stack((summary))\n","    summary = pd.DataFrame(data=summary,columns=[\"class\",\"count\",\"percentage\",\"model\"])\n","    summary[\"class\"] = summary[\"class\"].astype(str)\n","    summary[\"count\"] = summary[\"count\"].astype(int)\n","    summary[\"percentage\"] = summary[\"percentage\"].astype(float)\n","    summary = summary.round({\"percentage\":3})\n","\n","    return summary\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Init. Misc settings\n","# Setup the desired resampling Stratigy With 10 CV as Nested Inner Sampling\n","X_Data = input_df.drop([\"Landslides\"],axis=1,inplace=False).values\n","Y_Data = input_df[\"Landslides\"].values               \n","inner_cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n","outer_cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n","\n","# Split the Input data acooring the resampling strategy\n","X_train_out,Y_train_out,X_test_out,Y_test_out = cv_splitting(X_Data, Y_Data, outer_cv)\n","\n","# Evaluation metrics\n","score_metrics = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score),\n","                \"Kappa\":make_scorer(cohen_kappa_score)\n","                # \"Logloss\":make_scorer(log_loss,greater_is_better=False)\n","                }\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Tunning Random Forest"],"metadata":{}},{"source":["# # Init. model function for the objective function:\n","rf_clf = RandomForestClassifier()\n","\n","# # Setting up number of evaluations and Trials object\n","n_evals = 30\n","rf_trails = Trials()\n","\n","\n","rf_search_space = {\n","        'max_features': \"auto\",\n","        'n_estimators': hp_scope.int(hp.quniform('n_estimators', 32,512,1)),\n","        'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n","        }\n","\n","# # Running optimization\n","rf_tune = fmin(partial(objective_func, clf= rf_clf, xdata=X_train_out, ydata=Y_train_out,cv_resample=inner_cv),\n","                space=rf_search_space, algo=tpe.suggest,\n","                trials=rf_trails, max_evals=n_evals, rstate=rng)\n","\n","# Output the optimization history\n","rf_optimization_df, rf_best_hp = optimization_trails(rf_trails,rf_search_space)\n","\n","# Plot optimization history of Random Forest\n","fig, ax = optimization_history(rf_trails)\n","plt.show()\n","\n","# Generate final scores\n","rf_scores, rf_scores_raw  = tunned_model_evaluation(clf=rf_clf,opt_hp=space_eval(rf_search_space,rf_tune),x=X_Data,y=Y_Data,cv=outer_cv,scores=score_metrics)\n","\n","# Generate and Compute ROC curve\n","_, rf_roc_data = generate_roc_data(rf_clf.set_params(**space_eval(rf_search_space,rf_tune)),X_train_out,Y_train_out,X_test_out,Y_test_out)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Tunning Extra Trees"],"metadata":{}},{"source":["# # Init. model function for the objective function:\n","ext_clf = ExtraTreesClassifier()\n","\n","# # Setting up number of evaluations and Trials object\n","n_evals = 30\n","ext_trails = Trials()\n","\n","ext_search_space = {\n","        'max_features': \"auto\",\n","        'n_estimators': hp_scope.int(hp.quniform('n_estimators', 32,512,1)),\n","        'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n","        }\n","\n","# # Running optimization\n","ext_tune = fmin(partial(objective_func, clf= ext_clf, xdata=X_train_out, ydata=Y_train_out,cv_resample=inner_cv),\n","                space=ext_search_space, algo=tpe.suggest,\n","                trials=ext_trails, max_evals=n_evals, rstate=rng)\n","\n","# Output the optimization history\n","ext_optimization_df, ext_best_hp = optimization_trails(ext_trails,ext_search_space)\n","\n","# Plot optimization history of Random Forest\n","fig, ax = optimization_history(ext_trails)\n","plt.show()\n","\n","# Generate final scores\n","ext_scores, ext_scores_raw  = tunned_model_evaluation(clf=ext_clf,opt_hp=space_eval(ext_search_space,ext_tune),x=X_Data,y=Y_Data,cv=outer_cv,scores=score_metrics)\n","\n","# Generate and Compute ROC curve\n","_, ext_roc_data = generate_roc_data(ext_clf.set_params(**space_eval(ext_search_space,ext_tune)),X_train_out,Y_train_out,X_test_out,Y_test_out)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Tunning Passive Aggressive Algorithms"],"metadata":{}},{"source":["# # Init. model function for the objective function:\n","pa_clf = PassiveAggressiveClassifier()\n","\n","# # Setting up number of evaluations and Trials object\n","n_evals = 30\n","pa_trails = Trials()\n","\n","pa_search_space = {\n","        'C': hp.loguniform('C', -3,3),\n","        'early_stopping': True,\n","        'n_iter_no_change': 10,\n","        'max_iter': hp_scope.int(hp.quniform('max_iter', 128,1024,1)),\n","        'fit_intercept': hp.choice('fit_intercept', [0, 1])\n","        }\n","\n","# # Running optimization\n","pa_tune = fmin(partial(objective_func, clf= pa_clf, xdata=X_train_out, ydata=Y_train_out,cv_resample=inner_cv),\n","                space=pa_search_space, algo=tpe.suggest,\n","                trials=pa_trails, max_evals=n_evals, rstate=rng)\n","\n","# Output the optimization history\n","pa_optimization_df, pa_best_hp = optimization_trails(pa_trails,pa_search_space)\n","\n","# Plot optimization history of Random Forest\n","fig, ax = optimization_history(pa_trails)\n","plt.show()\n","\n","# Generate final scores\n","pa_scores, pa_scores_raw  = tunned_model_evaluation(clf=pa_clf,opt_hp=space_eval(pa_search_space,pa_tune),x=X_Data,y=Y_Data,cv=outer_cv,scores=score_metrics)\n","\n","# Generate and Compute ROC curve\n","_, pa_roc_data = generate_roc_data(pa_clf.set_params(**space_eval(pa_search_space,pa_tune)),X_train_out,Y_train_out,X_test_out,Y_test_out)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Tunning Support Vector Machine"],"metadata":{}},{"source":["# Init. model function for the objective function:\n","svm_clf = SVC(probability=True)\n","\n","# Setting up number of evaluations and Trials object\n","n_evals = 30\n","svm_trails = Trials()\n","\n","svm_search_space =  hp.choice('kernel',\n","                                      ({'kernel': 'linear', 'C': hp.loguniform('linear_C', -8, 8)}, \n","                                       {'kernel': 'rbf', 'C': hp.loguniform('rbf_C', -8, 8),\"gamma\": hp.loguniform('gamma', -8, 8)}))\n","ho_sample(svm_search_space)\n","# Running optimization\n","svm_tune = fmin(partial(objective_func, clf= svm_clf, xdata=X_train_out, ydata=Y_train_out,cv_resample=inner_cv),\n","                space=svm_search_space, algo=tpe.suggest,\n","                trials=svm_trails, max_evals=n_evals, rstate=rng)\n","\n","# # Output the optimization history\n","svm_optimization_df, svm_best_hp = optimization_trails(svm_trails,svm_search_space)\n","\n","# # Plot optimization history of Random Forest\n","fig, ax = optimization_history(svm_trails)\n","plt.show()\n","\n","# # Generate final scores\n","svm_scores, svm_scores_raw  = tunned_model_evaluation(clf=svm_clf,opt_hp=space_eval(svm_search_space,svm_tune),x=X_Data,y=Y_Data,cv=outer_cv,scores=score_metrics)\n","\n","# # Generate and Compute ROC curve\n","_, svm_roc_data = generate_roc_data(svm_clf.set_params(**space_eval(svm_search_space,svm_tune)),X_train_out,Y_train_out,X_test_out,Y_test_out)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Tunning Gaussian Naive Bayes"],"metadata":{}},{"source":["# Init. model function for the objective function:\n","# nb_clf = BernoulliNB()\n","nb_clf = GaussianNB()\n","\n","# Setting up number of evaluations and Trials object\n","n_evals = 2\n","nb_trails = Trials()\n","\n","nb_search_space =  {\n","        # 'alpha': hp.uniform('alpha', 0.0, 2.0)\n","        \"var_smoothing\": 1e-9\n","        }\n","\n","# Running optimization\n","nb_tune = fmin(partial(objective_func, clf= nb_clf, xdata=X_train_out, ydata=Y_train_out,cv_resample=inner_cv),\n","                space=nb_search_space, algo=tpe.suggest,\n","                trials=nb_trails, max_evals=n_evals, rstate=rng)\n","\n","# Output the optimization history\n","nb_optimization_df, nb_best_hp = optimization_trails(nb_trails,nb_search_space)\n","\n","# Plot optimization history of Random Forest\n","fig, ax = optimization_history(nb_trails)\n","plt.show()\n","\n","# Generate final scores\n","nb_scores, nb_scores_raw = tunned_model_evaluation(clf=nb_clf,opt_hp=space_eval(nb_search_space,nb_tune),x=X_Data,y=Y_Data,cv=outer_cv,scores=score_metrics)\n","\n","# Generate and Compute ROC curve\n","_, nb_roc_data = generate_roc_data(nb_clf.set_params(**space_eval(nb_search_space,nb_tune)),X_train_out,Y_train_out,X_test_out,Y_test_out)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Tunning Quadratic Analysis"],"metadata":{}},{"source":["# Init. model function for the objective function:\n","qd_clf = QuadraticDiscriminantAnalysis()\n","\n","# Setting up number of evaluations and Trials object\n","n_evals = 2\n","qd_trails = Trials()\n","\n","qd_search_space =  {\"tol\": 1e-4}\n","\n","# Running optimization\n","qd_tune = fmin(partial(objective_func, clf= qd_clf, xdata=X_train_out, ydata=Y_train_out,cv_resample=inner_cv),\n","                space=qd_search_space, algo=tpe.suggest,\n","                trials=qd_trails, max_evals=n_evals, rstate=rng)\n","\n","# Output the optimization history\n","qd_optimization_df, qd_best_hp = optimization_trails(qd_trails,qd_search_space)\n","\n","# Plot optimization history of Random Forest\n","fig, ax = optimization_history(qd_trails)\n","plt.show()\n","\n","# Generate final scores\n","qd_scores, qd_scores_raw = tunned_model_evaluation(clf=qd_clf,opt_hp=space_eval(qd_search_space,qd_tune),x=X_Data,y=Y_Data,cv=outer_cv,scores=score_metrics)\n","\n","# Generate and Compute ROC curve\n","_, qd_roc_data = generate_roc_data(qd_clf.set_params(**space_eval(qd_search_space,qd_tune)),X_train_out,Y_train_out,X_test_out,Y_test_out)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Tunning Logistic Regression"],"metadata":{}},{"source":["# Init. model function for the objective function:\n","lr_clf = LogisticRegression()\n","\n","# Setting up number of evaluations and Trials object\n","n_evals = 2\n","lr_trails = Trials()\n","\n","lr_search_space =  {\"C\": 1.0,\"solver\":'lbfgs'}\n","\n","# Running optimization\n","lr_tune = fmin(partial(objective_func, clf= lr_clf, xdata=X_train_out, ydata=Y_train_out,cv_resample=inner_cv),\n","                space=lr_search_space, algo=tpe.suggest,\n","                trials=lr_trails, max_evals=n_evals, rstate=rng)\n","\n","# Output the optimization history\n","lr_optimization_df, lr_best_hp = optimization_trails(lr_trails,lr_search_space)\n","\n","# Plot optimization history of Random Forest\n","fig, ax = optimization_history(lr_trails)\n","plt.show()\n","\n","# Generate final scores\n","lr_scores, lr_scores_raw = tunned_model_evaluation(clf=lr_clf,opt_hp=space_eval(lr_search_space,lr_tune),x=X_Data,y=Y_Data,cv=outer_cv,scores=score_metrics)\n","\n","# Generate and Compute ROC curve\n","_, lr_roc_data = generate_roc_data(lr_clf.set_params(**space_eval(lr_search_space,lr_tune)),X_train_out,Y_train_out,X_test_out,Y_test_out)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Tunning K-Nearest Neighbors"],"metadata":{}},{"source":["# Init. model function for the objective function:\n","knn_clf = KNeighborsClassifier()\n","\n","# Setting up number of evaluations and Trials object\n","n_evals = 30\n","knn_trails = Trials()\n","\n","knn_search_space =  {\n","        'n_neighbors': hp_scope.int(hp.quniform('knn_n_neighbors', 1, 50,1)),\n","        'p': hp_scope.int(hp.quniform('p', 1, 5,1)),\n","        'weights': hp.choice('weights', [\"uniform\",\"distance\"])\n","        }\n","\n","# Running optimization\n","knn_tune = fmin(partial(objective_func, clf= knn_clf, xdata=X_train_out, ydata=Y_train_out,cv_resample=inner_cv),\n","                space=knn_search_space, algo=tpe.suggest,\n","                trials=knn_trails, max_evals=n_evals, rstate=rng)\n","\n","# Output the optimization history\n","knn_optimization_df, knn_best_hp = optimization_trails(knn_trails,knn_search_space)\n","\n","# Plot optimization history of Random Forest\n","fig, ax = optimization_history(knn_trails)\n","plt.show()\n","\n","# Generate final scores\n","knn_scores, knn_scores_raw = tunned_model_evaluation(clf=knn_clf,opt_hp=space_eval(knn_search_space,knn_tune),x=X_Data,y=Y_Data,cv=outer_cv,scores=score_metrics)\n","\n","# Generate and Compute ROC curve\n","_, knn_roc_data = generate_roc_data(knn_clf.set_params(**space_eval(knn_search_space,knn_tune)),X_train_out,Y_train_out,X_test_out,Y_test_out)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Tunning Decision Trees"],"metadata":{}},{"source":["# Init. model function for the objective function:\n","dt_clf = DecisionTreeClassifier()\n","\n","# Setting up number of evaluations and Trials object\n","n_evals = 30\n","dt_trails = Trials()\n","\n","dt_search_space = {\n","        'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0,0.5),\n","        'max_features': hp.uniform('max_features', 0,1),\n","        'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n","        }\n","\n","# Running optimization\n","dt_tune = fmin(partial(objective_func, clf= dt_clf, xdata=X_train_out, ydata=Y_train_out,cv_resample=inner_cv),\n","                space=dt_search_space, algo=tpe.suggest,\n","                trials=dt_trails, max_evals=n_evals, rstate=rng)\n","\n","# Output the optimization history\n","dt_optimization_df, dt_best_hp = optimization_trails(dt_trails,dt_search_space)\n","\n","# Plot optimization history of Random Forest\n","fig, ax = optimization_history(dt_trails)\n","plt.show()\n","\n","# Generate final scores\n","dt_scores, dt_scores_raw = tunned_model_evaluation(clf=DecisionTreeClassifier(),opt_hp=space_eval(dt_search_space,dt_tune),x=X_Data,y=Y_Data,cv=outer_cv,scores=score_metrics)\n","\n","# Generate and Compute ROC curve\n","_, dt_roc_data = generate_roc_data(DecisionTreeClassifier(**space_eval(dt_search_space,dt_tune)),X_train_out,Y_train_out,X_test_out,Y_test_out)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Tunning Neural Network"],"metadata":{}},{"source":["# Init. model function for the objective function:\n","nnet_clf = MLPClassifier()\n","\n","# Setting up number of evaluations and Trials object\n","n_evals = 30\n","nnet_trails = Trials()\n","\n","nnet_search_space = {\n","        'hidden_layer_sizes': hp_scope.int(hp.quniform('hidden_layer_sizes', 3,54,1)),\n","        'activation': 'relu',\n","        'solver': 'lbfgs',\n","        'max_iter': 128\n","        }\n","\n","# Running optimization\n","nnet_tune = fmin(partial(objective_func, clf= nnet_clf, xdata=X_train_out, ydata=Y_train_out,cv_resample=inner_cv),\n","                space=nnet_search_space, algo=tpe.suggest,\n","                trials=nnet_trails, max_evals=n_evals, rstate=rng)\n","\n","# Output the optimization history\n","nnet_optimization_df, nnet_best_hp = optimization_trails(nnet_trails,nnet_search_space)\n","\n","# Plot optimization history of Random Forest\n","fig, ax = optimization_history(nnet_trails)\n","plt.show()\n","\n","# Generate final scores\n","nnet_scores, nnet_scores_raw = tunned_model_evaluation(clf=MLPClassifier(),opt_hp=space_eval(nnet_search_space,nnet_tune),x=X_Data,y=Y_Data,cv=outer_cv,scores=score_metrics)\n","\n","# Generate and Compute ROC curve\n","_, nnet_roc_data = generate_roc_data(MLPClassifier(**space_eval(nnet_search_space,nnet_tune)),X_train_out,Y_train_out,X_test_out,Y_test_out)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Tunning LightGBM"],"metadata":{}},{"source":["# # Init. model function for the objective function:\n","lgb_clf = LGBMClassifier()\n","\n","# # Setting up number of evaluations and Trials object\n","n_evals = 30\n","lgb_trails = Trials()\n","\n","lgb_search_space = {\n","        # \"max_depth\" : hp_scope.int(hp.quniform(\"max_depth\", 2,20, 1)),\n","        \"learning_rate\" : hp.uniform(\"learning_rate\", 1e-3, 1),\n","        #\"learning_rate\" : hp.uniform(\"learning_columnsrate\", 1e-3, 1),\n","        # \"subsample\" : hp.uniform(\"subsample\", 0.5, 1),\n","        \"num_leaves\" : hp_scope.int(hp.quniform(\"num_leaves\",4,1024,1)), # < 2**depth for 0.6  \n","        \"n_estimators\" : hp_scope.int(hp.quniform(\"n_estimators\",16,1024,1))\n","        }\n","\n","# # Running optimization\n","lgb_tune = fmin(partial(objective_func, clf= lgb_clf, xdata=X_train_out, ydata=Y_train_out,cv_resample=inner_cv),\n","                space=lgb_search_space, algo=tpe.suggest,\n","                trials=lgb_trails, max_evals=n_evals, rstate=rng)\n","\n","# Output the optimization history\n","lgb_optimization_df, lgb_best_hp = optimization_trails(lgb_trails,lgb_search_space)\n","\n","# Plot optimization history of Random Forest\n","fig, ax = optimization_history(lgb_trails)\n","plt.show()\n","\n","# Generate final scores\n","lgb_scores, lgb_scores_raw  = tunned_model_evaluation(clf=lgb_clf,opt_hp=space_eval(lgb_search_space,lgb_tune),x=X_Data,y=Y_Data,cv=outer_cv,scores=score_metrics)\n","\n","# Generate and Compute ROC curve\n","_, lgb_roc_data = generate_roc_data(lgb_clf.set_params(**space_eval(lgb_search_space,lgb_tune)),X_train_out,Y_train_out,X_test_out,Y_test_out)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Tunning Bart"],"metadata":{}},{"source":["# # Init. model function for the objective function:\n","from bartpy.sklearnmodel import SklearnModel\n","\n","bart_clf = SklearnModel()\n","\n","# # Setting up number of evaluations and Trials object\n","n_evals = 10\n","bart_trails = Trials()\n","\n","bart_search_space = {\n","        \"n_trees\" : hp_scope.int(hp.quniform(\"n_trees\", 16,256, 1)),\n","        # \"p_grow\" : hp.uniform(\"p_grow\", 0.5, 1),\n","        \"n_samples\" : hp_scope.int(hp.quniform(\"n_samples\",16,256,1)), # < 2**depth for 0.6  \n","        \"n_burn\" : hp_scope.int(hp.quniform(\"n_burn\",16,256,1))\n","        }\n","\n","# # Running optimization\n","bart_tune = fmin(partial(objective_func, clf= bart_clf, xdata=X_train_out, ydata=Y_train_out,cv_resample=inner_cv),\n","                space=bart_search_space, algo=tpe.suggest,\n","                trials=bart_trails, max_evals=n_evals, rstate=rng)\n","\n","# Output the optimization history\n","bart_optimization_df, bart_best_hp = optimization_trails(bart_trails,bart_search_space)\n","\n","# Plot optimization history of Random Forest\n","fig, ax = optimization_history(bart_trails)\n","plt.show()\n","\n","# Generate final scores\n","bart_scores, bart_scores_raw  = tunned_model_evaluation(clf=bart_clf,opt_hp=space_eval(bart_search_space,bart_tune),x=X_Data,y=Y_Data,cv=outer_cv,scores=score_metrics)\n","\n","# Generate and Compute ROC curve\n","_, bart_roc_data = generate_roc_data(bart_clf.set_params(**space_eval(bart_search_space,bart_tune)),X_train_out,Y_train_out,X_test_out,Y_test_out)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[""],"metadata":{}},{"source":["models_names_list = ['DT','EXT','KNN','LGB','LR','NB','NNET','QDA','RF','SVM']\n","models_raw_scores_list = [\n","                          dt_scores_raw[\"test_AUC\"],\n","                          ext_scores_raw[\"test_AUC\"],\n","                          knn_scores_raw[\"test_AUC\"],\n","                          lgb_scores_raw[\"test_AUC\"],\n","                          lr_scores_raw[\"test_AUC\"],\n","                          nb_scores_raw[\"test_AUC\"],\n","                          nnet_scores_raw[\"test_AUC\"],\n","                          qd_scores_raw[\"test_AUC\"],\n","                          rf_scores_raw[\"test_AUC\"],\n","                          svm_scores_raw[\"test_AUC\"]\n","                          ]\n","\n","sig_df = sig_stat(models_raw_scores_list,models_names_list)\n","with open(\"./Output/Tables/sig_test.md\",'w') as tabl:\n","    print(tabulate(sig_df, headers=['Pairwise', 'p.value','t.value'],tablefmt=\"grid\"),file=tabl)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["\n","\n",""],"metadata":{}},{"source":["models_roc_data_list = [\n","                          dt_roc_data,\n","                          ext_roc_data,\n","                          knn_roc_data,\n","                          lgb_roc_data,\n","                          lr_roc_data,\n","                          nb_roc_data,\n","                          nnet_roc_data,\n","                          qd_roc_data,\n","                          rf_roc_data,\n","                          svm_roc_data\n","                          ]\n","\n","fig, ax = plot_roc_curves(models_roc_data_list,models_names_list)\n","ax.set_xlim(0, 1.01)\n","ax.set_ylim(0, 1.01)\n","\n","plt.legend(loc=\"best\")\n","fig.tight_layout()\n","plt.show()\n","fig.savefig(\"./Output/Figures/ROC_plot1.jpg\",dpi=600, quality=100,bbox_inchs=\"tight\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["\n",""],"metadata":{}},{"source":["models_list = [\n","               dt_clf.set_params(**space_eval(dt_search_space,dt_tune)),\n","               ext_clf.set_params(**space_eval(ext_search_space,ext_tune)),\n","               knn_clf.set_params(**space_eval(knn_search_space,knn_tune)),\n","               lgb_clf.set_params(**space_eval(lgb_search_space,lgb_tune)),\n","               lr_clf.set_params(**space_eval(lr_search_space,lr_tune)),\n","               nb_clf.set_params(**space_eval(nb_search_space,nb_tune)),\n","               nnet_clf.set_params(**space_eval(nnet_search_space,nnet_tune)),\n","               qd_clf.set_params(**space_eval(qd_search_space,qd_tune)),\n","               rf_clf.set_params(**space_eval(rf_search_space,rf_tune)),\n","               svm_clf.set_params(**space_eval(svm_search_space,svm_tune))\n","               ]\n","\n","fig, ax = plot_calibration_curves(clf=models_list,x=X_Data ,y=Y_Data, test_size=0.3, cv = 10, seed=seed, clf_name=models_names_list)\n","ax.set_xlim(0, 1.01)\n","ax.set_ylim(0, 1.01)\n","\n","plt.legend(loc=\"best\")\n","fig.tight_layout()\n","plt.show()\n","fig.savefig(\"./Output/Figures/Calibration_plot1.jpg\",dpi=600, quality=100,bbox_inchs=\"tight\")\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["\n","\n",""],"metadata":{}},{"source":["st_data_grid = pd.read_hdf('./Data/Predict/Study_Area45_Array.hdf').iloc[:,0:16]\n","st_xy_grid = pd.read_hdf('./Data/Predict/Study_Area45_Array.hdf').values[:,-2:]\n","bins= [0.05,0.30,0.50,0.75,np.inf]\n","bins_labels=[\"Very Low\", \"Low\", \"Moderate\", \"High\", \"Very High\"]\n","bins_labels_replace = {\"0\":\"Very Low\", \"1\":\"Low\", \"2\":\"Moderate\", \"3\":\"High\", \"4\":\"Very High\",\n","                        \"0.0\":\"Very Low\", \"1.0\":\"Low\", \"2.0\":\"Moderate\", \"3.0\":\"High\", \"4.0\":\"Very High\"}\n","_, prediction_data, prediction_summary = perform_prediction(clf=models_list,clf_names=models_names_list,\\\n","train_data=[X_Data, Y_Data],predict_data=st_data_grid,\\\n","bins=bins,bins_labels=bins_labels,XY=st_xy_grid,export_prediction=False,export_csv=True)\n","prediction_summary = prediction_summary.replace({\"class\":bins_labels_replace})\n","\n","vrt_file = \"./Output/Tables/prediction.vrt\"\n","outputBounds,width, height = [305739,4054353,224105,4007733], 1632.68, 932.4\n","prediction_to_raster(clf_names=models_names_list,vrt_file=vrt_file,\n","                    layers=\"prediction\",outputSRS=\"EPSG:32632\",\n","                    outputBounds=outputBounds,width=width,height=height,noData=-9999)\n","sample_shp = \"./Data/shp/Ls_Pts.shp\"\n","rasters_list = [\"./Output/Figures/\" + model + \".tif\" for model in models_names_list]\n","prediction_overly_summary = extract_rs_unique_values(sample_shp, rasters_list,models_names_list)\n","prediction_overly_summary = prediction_overly_summary.replace({\"class\":bins_labels_replace})\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df = prediction_summary.pivot(\"model\",\"class\",\"percentage\")\n","df = df.reindex([\"Very Low\",\"Low\",\"Moderate\",\"High\",\"Very High\"],axis=1)\n","df.to_csv(\"./Output/Tables/prediction_summary_pivot.csv\",index=True,header=True)\n","df = prediction_summary.pivot(\"model\",\"class\",\"percentage\")\n","prediction_summary.to_csv(\"./Output/Tables/prediction_summary.csv\",index=None,header=True)\n","\n","fig, ax1 = plt.subplots(figsize=(7,7))\n","bar_width = 0.375\n","y_pos = np.arange(0,2*len(df),2)\n","yticks = len(df.columns)*bar_width/2 + y_pos\n","\n","for i, col in enumerate(df.columns):\n","    y_pos + i * bar_width\n","    bars = ax1.barh(y_pos + i * bar_width, df[col],\n","    height = bar_width,\n","    label = col)\n","    for bar in bars:\n","        # print(bar.get_width())\n","        ax1.annotate(f\"{round(bar.get_width(),2)}%\",\n","                xy=(bar.get_width() + 5,\n","                    bar.get_y() + bar.get_height()/2\n","                ),\n","                ha=\"center\", va='center',\n","                color=bar.get_facecolor(),size=10,fontweight=\"medium\")\n","ax1.legend(loc=\"upper right\")\n","# Add some text for labels, title and custom x-axis tick labels, etc.\n","ax1.set_xlabel(\"Area Extent (%)\")\n","ax1.set_ylabel(\"\")\n","ax1.set_yticks(yticks)\n","ax1.set_yticklabels(df.index)\n","ax1.set_ylim([0 - bar_width,20])\n","ax1.set_xlim([-0.05,50.5])\n","ax1.xaxis.grid(False)\n","ax1.yaxis.grid(False)\n","# Save the figure and show\n","fig.tight_layout()\n","plt.tight_layout()\n","fig.savefig(\"./Output/Figures/area_extent.jpg\",dpi=600, quality=100,bbox_inchs=\"tight\")\n","\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","df = prediction_overly_summary.pivot(\"model\",\"class\",\"percentage\")\n","df = df.reindex([\"Very Low\",\"Low\",\"Moderate\",\"High\",\"Very High\"],axis=1)\n","df.to_csv(\"./Output/Tables/prediction_overly_summary_pivot.csv\",index=True,header=True)\n","prediction_overly_summary.to_csv(\"./Output/Tables/prediction_overly_summary.csv\",index=None,header=True)\n","\n","fig, ax1 = plt.subplots(figsize=(7,7))\n","bar_width = 0.375\n","y_pos = np.arange(0,2*len(df),2)\n","yticks = len(df.columns)*bar_width/2 + y_pos\n","\n","for i, col in enumerate(df.columns):\n","    y_pos + i * bar_width\n","    bars = ax1.barh(y_pos + i * bar_width, df[col],\n","    height = bar_width,\n","    label = col)\n","    for bar in bars:\n","        # print(bar.get_width())\n","        ax1.annotate(f\"{round(bar.get_width(),2)}%\",\n","                xy=(bar.get_width() + 5,\n","                    bar.get_y() + bar.get_height()/2\n","                ),\n","                ha=\"center\", va='center',\n","                color=bar.get_facecolor(),size=10,fontweight=\"medium\")\n","ax1.legend(loc=\"upper right\")\n","# Add some text for labels, title and custom x-axis tick labels, etc.\n","ax1.set_xlabel(\"Landslide Distribution (%)\")\n","ax1.set_ylabel(\"\")\n","ax1.set_yticks(yticks)\n","ax1.set_yticklabels(df.index)\n","ax1.set_ylim(0-bar_width,20)\n","ax1.set_xlim([-0.05,102.5])\n","ax1.xaxis.grid(False)\n","ax1.yaxis.grid(False)\n","# Save the figure and show\n","fig.tight_layout()\n","plt.tight_layout()\n","fig.savefig(\"./Output/Figures/ls_distr.jpg\",dpi=600, quality=100,bbox_inchs=\"tight\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":[],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Output the Optimium parameters as md table\n","hyper_opt = pd.concat([rf_best_hp.T,svm_best_hp.T,nb_best_hp.T,knn_best_hp.T,dt_best_hp.T,nnet_best_hp.T,lgb_best_hp.T])\n","hyper_opt = pd.concat([\n","                        #   bart_best_hp.T,\n","                          dt_best_hp.T,\n","                          ext_best_hp.T,\n","                          knn_best_hp.T,\n","                          lgb_best_hp.T,\n","                          lr_best_hp.T,\n","                          nb_best_hp.T,\n","                          nnet_best_hp.T,\n","                        #   pa_best_hp.T,\n","                          qd_best_hp.T,\n","                          rf_best_hp.T,\n","                          svm_best_hp.T\n","                          ])\n","\n","hyper_opt = hyper_opt.rename(columns={0: \"optimium\",\"0\": \"optimium\"})\n","hyper_opt[\"hyperparamter\"] = hyper_opt.index\n","hyper_opt = hyper_opt.round({\"optimium\":4})\n","with open(\"./Output/Tables/hyper_opt.md\",'w') as tabl:\n","    print(tabulate(hyper_opt, headers=['Hyperparamter', 'Optimium'],tablefmt=\"grid\"),file=tabl)"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Output the scores results as md table\n","score_opt = pd.concat([\n","dt_scores.loc[[\"test_AUC\",\"test_Accuracy\",\"test_Kappa\"],[\"mean\",\"std\"]].T,\n","ext_scores.loc[[\"test_AUC\",\"test_Accuracy\",\"test_Kappa\"],[\"mean\",\"std\"]].T,\n","knn_scores.loc[[\"test_AUC\",\"test_Accuracy\",\"test_Kappa\"],[\"mean\",\"std\"]].T,\n","lgb_scores.loc[[\"test_AUC\",\"test_Accuracy\",\"test_Kappa\"],[\"mean\",\"std\"]].T,\n","lr_scores.loc[[\"test_AUC\",\"test_Accuracy\",\"test_Kappa\"],[\"mean\",\"std\"]].T,\n","nb_scores.loc[[\"test_AUC\",\"test_Accuracy\",\"test_Kappa\"],[\"mean\",\"std\"]].T,\n","nnet_scores.loc[[\"test_AUC\",\"test_Accuracy\",\"test_Kappa\"],[\"mean\",\"std\"]].T,\n","qd_scores.loc[[\"test_AUC\",\"test_Accuracy\",\"test_Kappa\"],[\"mean\",\"std\"]].T,\n","rf_scores.loc[[\"test_AUC\",\"test_Accuracy\",\"test_Kappa\"],[\"mean\",\"std\"]].T,\n","svm_scores.loc[[\"test_AUC\",\"test_Accuracy\",\"test_Kappa\"],[\"mean\",\"std\"]].T\n","])\n","score_opt = score_opt.round({\"test_AUC\":3,\"test_Accuracy\":3,\"test_Kappa\":3})\n","with open(\"./Output/Tables/score_opt.md\",'w') as tabl:\n","    print(tabulate(score_opt, headers=['AUC', 'ACC','Kappa'],tablefmt=\"grid\"),file=tabl)\n","\n","# Build the plot\n","# Define labels, positions, bar heights and error bar heights\n","for idx, metric in enumerate([\"AUC\",\"ACC\",\"Kappa\"]):\n","    # models_names_list = [\"RF\",\"SVM\",\"NB\",\"KNN\",\"DT\",\"NNET\",\"LGB\"]\n","    y_pos = np.arange(len(models_names_list))\n","    ct = [score_opt.iloc[i,idx] for i in range(0,len(score_opt),2)]\n","    error = [score_opt.iloc[i,idx] for i in range(1,len(score_opt),2)]\n","    plt.style.use(\"seaborn-paper\")\n","    fig, ax = plt.subplots(figsize=(5,5))\n","    ax.barh(y_pos,ct,\n","        xerr=error,\n","        align='center',\n","        alpha=0.5,\n","        ecolor='black',\n","        capsize=10)\n","    ax.set_xlabel(metric)\n","    ax.set_yticks(y_pos)\n","    ax.set_yticklabels(models_names_list)\n","    # ax.set_title('Coefficent of Thermal Expansion (CTE) of Three Metals')\n","    ax.yaxis.grid(False)\n","    ax.xaxis.grid(False)\n","\n","    # Save the figure and show\n","    fig.tight_layout()\n","    plt.tight_layout()\n","    plt.savefig('./Output/Figures/'+metric+'.jpg',quality=100,dpi=300,bbox_inchs=\"tight\")\n","    plt.show()\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}